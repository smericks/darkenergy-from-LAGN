{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lenstronomy\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# lenstronomy stuff\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from lenstronomy.LensModel.lens_model import LensModel\n",
    "from lenstronomy.PointSource.point_source import PointSource\n",
    "from lenstronomy.Plots import lens_plot\n",
    "from lenstronomy.LensModel.Solver.lens_equation_solver import LensEquationSolver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- need to start with defining the ground truth? \n",
    "- you can have ground truth lens parameters, ground truth H_0, \n",
    "- a table of ground truth params, predicted params, etc?\n",
    "\n",
    "1. define ground truth lens params (either from a catalog or generate randomly)\n",
    "2. construct emulated posteriors for those lens params\n",
    "3. define ground truth H-0\n",
    "4. compute ground truth time delays\n",
    "5. constructed emulated time delay measurement\n",
    "6. construct Ddt posteriors from lens param posterior + time delay measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_doppel_seq_list = np.load('/Users/smericks/Desktop/StrongLensing/STRIDES14results/paper_results/doppelganger_test_set/results/y_pred_list_epoch10.npy')\n",
    "std_pred_doppel_seq_list = np.load('/Users/smericks/Desktop/StrongLensing/STRIDES14results/paper_results/doppelganger_test_set/results/std_pred_list_epoch10.npy')\n",
    "\n",
    "# load in predictions from broad training\n",
    "file_prefix = '/Users/smericks/Desktop/StrongLensing/STRIDES14results/paper_results/broad_training/diag_no_R_src/J2145_rerun_take2/'\n",
    "file_prefix = '/Users/smericks/Desktop/StrongLensing/STRIDES14results/paper_results/broad_training/diag_no_R_src/J2145_rerun_take2/'\n",
    "file_path = file_prefix+'doppelganger_predictions.h5'\n",
    "h5f = h5py.File(file_path, 'r')\n",
    "y_test_doppel = h5f.get('y_test')[:]\n",
    "h5f.close()\n",
    "\n",
    "# ordering is 0x,1x,2x,4x\n",
    "y_pred_doppel = y_pred_doppel_seq_list[2]\n",
    "std_pred_doppel = std_pred_doppel_seq_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lens_sample\n",
    "\n",
    "my_sample = lens_sample.LensSample(y_test_doppel,y_pred_doppel,std_pred_doppel,param_indices=[0,3,4,5,6,7,1,2,8,9])\n",
    "my_sample.populate_image_positions()\n",
    "my_sample.populate_fermat_differences()\n",
    "my_sample.populate_redshifts()\n",
    "# what if I populate the actual redshifts instead? That will change the ground \n",
    "# truth time delays & more accurately reflect the selection function\n",
    "doppel_folders = ['ATLASJ2344-3056', 'J0029-3814', 'J2205-3727', \n",
    "    'SDSSJ1251+2935', 'WISEJ0259-1635', 'DESJ0405-3308','J1131-4419',\n",
    "    'PSJ1606-2333', 'W2MJ1042+1641', 'DESJ0420-4037', 'J2145+6345', \n",
    "    'SDSSJ0248+1913', 'WG0214-2105']\n",
    "for r in range(0,len(my_sample.lens_df)):\n",
    "    metadata_path = ('/Users/smericks/Desktop/StrongLensing/deep-lens-modeling/'+\n",
    "        'paper/doppelganger_test_set/doppel_images/'+doppel_folders[r]+'/metadata.csv')\n",
    "    paltas_config = pd.read_csv(metadata_path)\n",
    "    # lens redshift\n",
    "    my_sample.lens_df.loc[r,'z_lens'] = paltas_config.loc[0,'main_deflector_parameters_z_lens']\n",
    "    # source redshift\n",
    "    my_sample.lens_df.loc[r,'z_src'] = paltas_config.loc[0,'source_parameters_z_source']\n",
    "\n",
    "my_sample.populate_truth_Ddt()\n",
    "my_sample.populate_truth_time_delays()\n",
    "my_sample.populate_measured_time_delays()\n",
    "my_sample.lens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try out fpd samps vs. truth\n",
    "\n",
    "for j in range(0,13):\n",
    "\n",
    "    pred_samples = my_sample.pred_fpd_samples(j)\n",
    "    truth = my_sample.lens_df.iloc[j][['fpd01', 'fpd02', 'fpd03']]\n",
    "\n",
    "    fig,axs = plt.subplots(1,3,figsize=(10,5))\n",
    "    for i in range(0,3):\n",
    "        axs[i].hist(pred_samples[i],label='predicted')\n",
    "        axs[i].vlines(truth[i],0,200,color='red',label='truth')\n",
    "        axs[i].set_xlabel('$\\Delta\\phi_{0%d}$'%(i+1))\n",
    "        if i == 2:\n",
    "            axs[i].legend()\n",
    "\n",
    "    plt.suptitle('Doppelganger %d'%(j))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,13):\n",
    "    Ddt_samps, Ddt_likelihoods = my_sample.Ddt_posterior(i,td_uncertainty=2)\n",
    "    binwidth = 100\n",
    "    plt.hist(Ddt_samps,weights=Ddt_likelihoods,label='Ddt posterior',bins=np.arange(0,15000+binwidth,binwidth))\n",
    "    plt.vlines(my_sample.lens_df.loc[i,'Ddt_Mpc_truth'],0,0.03,color='red',label='truth')\n",
    "    plt.title('Ddt Doppelganger %d'%(i))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,13):\n",
    "    h0_samps, h0_weights = my_sample.H0_individual_lens(i)\n",
    "    binwidth = 1\n",
    "    n,_,_ = plt.hist(h0_samps,weights=h0_weights,label='H0 posterior',bins=np.arange(0,150+binwidth,binwidth))\n",
    "    plt.vlines(70.,0,np.max(n),color='red',label='truth')\n",
    "    plt.title('H0 Doppelganger %d'%(i))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do one joint constraint on H0 with all the lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_samps, h0_weights = my_sample.H0_joint_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference_utils\n",
    "from plot_utils import PALETTE\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "binwidth = 1\n",
    "n,_,_ = plt.hist(h0_samps,weights=h0_weights,label='Posterior',\n",
    "    bins=np.arange(0,150+binwidth,binwidth),color=PALETTE['medium_purple'])\n",
    "plt.vlines(70.,0,np.max(n),color='black',label='Truth',linestyle='--')\n",
    "plt.title('13 Doppelgangers')\n",
    "plt.legend(fontsize=13)\n",
    "plt.xlabel('$H_0$ (km/s/Mpc)',fontsize=13)\n",
    "plt.ylabel('Probability Density',fontsize=13)\n",
    "h0_mean,h0_sigma = inference_utils.median_sigma_from_samples(h0_samps,h0_weights)\n",
    "plt.text(80,0.038,'$H_0$ = %.1f +/- %.1f'%(h0_mean,h0_sigma),color=PALETTE['medium_purple'],fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- for time delays, let's assert 5 days error for 5 season measurement, 2 days for 10 season measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lens_catalog import OM10LensCatalog\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils.inference_utils import median_sigma_from_samples\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import corner\n",
    "from matplotlib.lines import Line2D\n",
    "import Utils.mcmc_utils as mcmc_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lenses = OM10LensCatalog('MassModels/om10_sample/om10_venkatraman_erickson24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Gold/Silver Samples ###\n",
    "\n",
    "We split the sample into doubles and quads. We already have a sample of ~30 quads (the STRIDES sample), so we take a conservative assumption of 50 quads in the gold sample. We add 200 doubles to the gold sample.\n",
    "The remaining lenses are added to the silver sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into doubles & quads\n",
    "dbls = gt_lenses.doubles_indices()\n",
    "quads = gt_lenses.quads_indices()\n",
    "# Let's assume in the gold sample: 200 doubles, 50 quads (overall quad fraction is 11%, so this is amplified)\n",
    "# The rest are silver (regardless of time delay)\n",
    "dbls = gt_lenses.doubles_indices()\n",
    "quads = gt_lenses.quads_indices()\n",
    "\n",
    "gold_dbls = dbls[:200]\n",
    "silver_dbls = dbls[200:]\n",
    "\n",
    "gold_quads = quads[:50]\n",
    "silver_quads = quads[50:]\n",
    "\n",
    "gold = np.append(gold_dbls,gold_quads)\n",
    "silver = np.append(silver_dbls,silver_quads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Data Vectors ###\n",
    "\n",
    "This is all done using the make_data_vectors() function in make_data_vectors.py\n",
    "\n",
    "1. Emulates time delay measurements\n",
    "2. Uses NPE posteriors to compute samples from fermat potential difference posteriors\n",
    "3. Stores all information in format for fast_tdc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_data_vectors import make_data_vectors\n",
    "\n",
    "# load in NPE posteriors\n",
    "hst_mu = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/y_pred_hst.npy')\n",
    "hst_cov = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/cov_pred_hst.npy')\n",
    "\n",
    "lsst_mu = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/y_pred_lsst.npy')\n",
    "lsst_cov = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/cov_pred_lsst.npy')\n",
    "\n",
    "# where to store this test\n",
    "exp_folder = 'gold2days_test'\n",
    "\n",
    "# gold quads\n",
    "make_data_vectors(gt_lenses,gold_quads,num_images=4,td_meas_error=2,\n",
    "    npe_mu=hst_mu,npe_cov=hst_cov,\n",
    "    h5_save_path=('DataVectors/'+exp_folder+'/gold_quads.h5'),\n",
    "    num_fpd_samps=1000)\n",
    "\n",
    "# gold dbls\n",
    "make_data_vectors(gt_lenses,gold_dbls,num_images=2,td_meas_error=2,\n",
    "    npe_mu=hst_mu,npe_cov=hst_cov,\n",
    "    h5_save_path=('DataVectors/'+exp_folder+'/gold_dbls.h5'),\n",
    "    num_fpd_samps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check chains from make_data_vectors() test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('DataVectors/gold2days_test/gold_chain1000.h5', 'r')\n",
    "gold_chain1 = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('DataVectors/gold2days_silver5days/gold_chain1000.h5', 'r')\n",
    "gold_chain2 = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float))\n",
    "std_mean_gold = np.std(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "\n",
    "exp_chains = [gold_chain1,gold_chain2]#mcmc_chain_quads_w0wa, mcmc_chain_quads_w0wa_1day,mcmc_chain_quads_w0wa_1day_EM, mcmc_chain_quads_w0wa_200]\n",
    "exp_names = ['NEW code: Gold: 50 Quads+ 200 Dbls, NPE Models, 1e3 fpd samps', #2-day td error\n",
    "             'OLD code: Gold: 50 Quads+ 200 Dbls, NPE Models, 1e3 fpd samps',\n",
    "             '250Gold+1034Silver, NPE Models, 5-day silver TD error']#'50 Gold Quads, NPE Models, 2-day td error', \n",
    "             #'50 Gold Quads, NPE Models, 1-day td error', \n",
    "             #'50 Gold Quads, Emulated Models, 1-day td error',\n",
    "             #'Gold: 50 Quads+ 200 Dbls, NPE Models, 1-day td error']\n",
    "burnin = int(1e2)\n",
    "colors = ['purple','turquoise']#'gold','silver','indianred','turquoise','purple']\n",
    "custom_labels = []\n",
    "\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin:,:].reshape((-1,5)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,-1.,0.,mu_mean_gold,std_mean_gold],truth_color='black',\n",
    "            fig=None,label_kwargs={'fontsize':17})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin:,:].reshape((-1,5)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,-1.,0.,mu_mean_gold,std_mean_gold],truth_color='black',\n",
    "            fig=figure,label_kwargs={'fontsize':17})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((5, 5))\n",
    "axes[0,4].legend(custom_lines,custom_labels,frameon=False,fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look further into chains (debugging option) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_labels = np.asarray(['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'])\n",
    "true_hyperparameters = np.asarray([70.,-1.,0.,mu_mean_gold,std_mean_gold])\n",
    "\n",
    "mcmc_utils.analyze_chains(gold_chain1,param_labels,true_hyperparameters,\n",
    "                    'test.txt',show_chains=True,\n",
    "                    burnin=int(1e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the assumed training prior match the effective training prior after cuts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_val = pd.read_csv('MassModels/hst_validation_metadata.csv')\n",
    "gamma_vals = hst_val['main_deflector_parameters_gamma'].to_numpy().astype(float)\n",
    "plt.hist(gamma_vals,density=True,label='Samples')\n",
    "x_range = np.arange(1.4,2.6,0.01)\n",
    "mu_est = np.mean(gamma_vals)\n",
    "sigma_est = np.std(gamma_vals,ddof=1)\n",
    "plt.plot(x_range,norm.pdf(x_range,mu_est,sigma_est),label='$\\mu=%.3f$, $\\sigma=%.2f$'%(mu_est,sigma_est))\n",
    "plt.legend()\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.figure()\n",
    "hst_train0 = pd.read_csv('MassModels/hst_train0_metadata.csv')\n",
    "gamma_vals = hst_train0['main_deflector_parameters_gamma'].to_numpy().astype(float)\n",
    "plt.hist(gamma_vals,density=True,label='Samples')\n",
    "x_range = np.arange(1.4,2.6,0.01)\n",
    "mu_est = np.mean(gamma_vals)\n",
    "sigma_est = np.std(gamma_vals,ddof=1)\n",
    "plt.plot(x_range,norm.pdf(x_range,mu_est,sigma_est),label='$\\mu=%.3f$, $\\sigma=%.2f$'%(mu_est,sigma_est))\n",
    "plt.legend()\n",
    "plt.title('Train 0')\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import gaussian_kde\n",
    "plt.figure()\n",
    "\n",
    "gaussian_samples = norm.rvs(mu_est,sigma_est,size=10000)\n",
    "\n",
    "# Let's try a KDE\n",
    "gamma_kde = gaussian_kde(gamma_vals)\n",
    "kde_samples = gamma_kde.resample(size=10000)[0]\n",
    "bins=np.histogram(np.hstack((kde_samples,gamma_vals)), bins=40)[1]\n",
    "\n",
    "counts_exp,_,_ = plt.hist(kde_samples,bins,\n",
    "            histtype='step',label='KDE Estimate')\n",
    "counts_obs,_,_ = plt.hist(gamma_vals,bins,histtype='step',\n",
    "                          label='Training Samples')\n",
    "    \n",
    "# only take bins where counts are greater than 5, then add in an extra bin at beginning and end for the tails\n",
    "idx = np.where((counts_exp > 5) & (counts_obs > 5))[0]\n",
    "# less than some #, bins, greather than some #\n",
    "prepend = np.sum(counts_obs[:idx[0]])\n",
    "append = np.sum(counts_obs[(idx[-1]+1):])\n",
    "counts_obs_final = np.concatenate(([prepend],counts_obs[idx],[append]))\n",
    "prepend = np.sum(counts_exp[:idx[0]])\n",
    "append = np.sum(counts_exp[(idx[-1]+1):])\n",
    "counts_exp_final = np.concatenate(([prepend],counts_exp[idx],[append]))\n",
    "\n",
    "\n",
    "\n",
    "chi2_distance = np.sum((counts_obs_final-counts_exp_final)**2/counts_exp_final)\n",
    "#print(chi2_distance)\n",
    "\n",
    "chi2,_= chisquare(counts_obs_final,counts_exp_final)\n",
    "print(chi2)\n",
    "\n",
    "dof = len(counts_obs_final) - 1\n",
    "print('chi2/dof:', chi2/dof)\n",
    "plt.text(1.6,800,r'$\\frac{\\chi^2}{\\nu}$: %.2f'%(chi2/dof),\n",
    "                        {'fontsize':13})\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

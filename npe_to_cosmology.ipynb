{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lens_catalog import OM10LensCatalog\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils.inference_utils import median_sigma_from_samples\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import corner\n",
    "from matplotlib.lines import Line2D\n",
    "import Utils.mcmc_utils as mcmc_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lenses = OM10LensCatalog('MassModels/om10_sample/om10_venkatraman_erickson24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Gold/Silver Samples ###\n",
    "\n",
    "We split the sample into doubles and quads. We already have a sample of ~30 quads (the STRIDES sample), so we take a conservative assumption of 50 quads in the gold sample. We add 200 doubles to the gold sample.\n",
    "The remaining lenses are added to the silver sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into doubles & quads\n",
    "dbls = gt_lenses.doubles_indices()\n",
    "quads = gt_lenses.quads_indices()\n",
    "# Let's assume in the gold sample: 200 doubles, 50 quads (overall quad fraction is 11%, so this is amplified)\n",
    "# The rest are silver (regardless of time delay)\n",
    "dbls = gt_lenses.doubles_indices()\n",
    "quads = gt_lenses.quads_indices()\n",
    "\n",
    "gold_dbls = dbls[:200]\n",
    "silver_dbls = dbls[200:]\n",
    "\n",
    "gold_quads = quads[:50]\n",
    "silver_quads = quads[50:]\n",
    "\n",
    "gold = np.append(gold_dbls,gold_quads)\n",
    "silver = np.append(silver_dbls,silver_quads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Emulate\" Time Delay Measurements ###\n",
    "\n",
    "We assume a time delay measurement uncertainty of 2 days for the gold lenses (dedicated monitoring campaigns), and 5 days for the silver lenses (LSST light curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "gold_sigma = 2.\n",
    "silver_sigma = 5.\n",
    "# gold doubles 'measured' time-delays\n",
    "mus = gt_lenses.lens_df.loc[gold_dbls,'td01']\n",
    "sigma = np.ones(len(mus))*gold_sigma\n",
    "gold_dbls_measured_td = np.zeros((len(mus),3))\n",
    "gold_dbls_measured_td[:,0] = norm.rvs(loc=mus,scale=sigma)\n",
    "gold_dbls_measured_prec = np.zeros((len(mus),3,3))\n",
    "gold_dbls_measured_prec[:,0,0] += 1/(gold_sigma**2)\n",
    "gold_dbls_prefactor = np.ones(len(mus))* (1/(2*np.pi))**(1/2) / gold_sigma\n",
    "\n",
    "# silver doubles 'measured' time-delays\n",
    "mus = gt_lenses.lens_df.loc[silver_dbls,'td01']\n",
    "sigma = np.ones(len(mus))*silver_sigma\n",
    "silver_dbls_measured_td = np.zeros((len(mus),3))\n",
    "silver_dbls_measured_td[:,0] = norm.rvs(loc=mus,scale=sigma)\n",
    "silver_dbls_measured_prec = np.zeros((len(mus),3,3))\n",
    "silver_dbls_measured_prec[:,0,0] += 1/(silver_sigma**2)\n",
    "silver_dbls_prefactor = np.ones(len(mus))* (1/(2*np.pi))**(1/2) / gold_sigma\n",
    "\n",
    "# gold quads 'measured' time-delays\n",
    "mus = gt_lenses.lens_df.loc[gold_quads,['td01','td02','td03']].to_numpy()\n",
    "sigma = np.ones(len(mus))*gold_sigma\n",
    "gold_quads_measured_td = np.ones((len(mus),3))*np.nan\n",
    "for i in range(0,3):\n",
    "    gold_quads_measured_td[:,i] = norm.rvs(loc=mus[:,i],scale=sigma)\n",
    "gold_quads_measured_prec = np.eye(3,3)/(gold_sigma**2)\n",
    "gold_quads_measured_prec = np.repeat(gold_quads_measured_prec[np.newaxis,:],len(mus), axis=0)\n",
    "gold_quads_prefactor = np.ones(len(mus))* (1/(2*np.pi))**(3/2) / np.sqrt(np.linalg.det(np.eye(3,3)*(gold_sigma**2)))\n",
    "\n",
    "# silver quads 'measured' time-delays\n",
    "mus = gt_lenses.lens_df.loc[silver_quads,['td01','td02','td03']].to_numpy()\n",
    "sigma = np.ones(len(mus))*silver_sigma\n",
    "silver_quads_measured_td = np.ones((len(mus),3))*np.nan\n",
    "for i in range(0,3):\n",
    "    silver_quads_measured_td[:,i] = norm.rvs(loc=mus[:,i],scale=sigma)\n",
    "silver_quads_measured_prec = np.eye(3,3)/(silver_sigma**2)\n",
    "silver_quads_measured_prec = np.repeat(silver_quads_measured_prec[np.newaxis,:],len(mus), axis=0)\n",
    "silver_quads_prefactor = np.ones(len(mus))* (1/(2*np.pi))**(3/2) / np.sqrt(np.linalg.det(np.eye(3,3)*(silver_sigma**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPD & Gamma Samples ###\n",
    "\n",
    "Using NPE mass models, return samples of fermat potential difference and $\\gamma_{lens}$. Two different networks are trained for HST quality images (gold) and LSST quality images (silver). We'll track 5,000 samples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in pre-computed NPE preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_mu = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/y_pred_hst.npy')\n",
    "hst_cov = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/cov_pred_hst.npy')\n",
    "\n",
    "lsst_mu = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/y_pred_lsst.npy')\n",
    "lsst_cov = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/cov_pred_lsst.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute fpd samps,  track gamma samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import batched_fermatpot\n",
    "num_samps = 5000\n",
    "# gold doubles\n",
    "gold_dbls_fpd_samps = np.zeros((len(gold_dbls),num_samps,3))\n",
    "gold_dbls_gamma_samps = np.empty((len(gold_dbls),num_samps))\n",
    "for i in range(0,len(gold_dbls)):\n",
    "    lens_param_samps = multivariate_normal.rvs(mean=hst_mu[gold_dbls[i]],\n",
    "        cov=hst_cov[gold_dbls[i]],size=num_samps)\n",
    "    x_im = gt_lenses.lens_df.loc[gold_dbls[i],['x_im0','x_im1']].to_numpy().astype(float)\n",
    "    y_im = gt_lenses.lens_df.loc[gold_dbls[i],['y_im0','y_im1']].to_numpy().astype(float)\n",
    "    fpd_samps = batched_fermatpot.eplshear_fp_samples(x_im,y_im,\n",
    "        lens_param_samps[:,:8],lens_param_samps[:,8],lens_param_samps[:,9])\n",
    "    gold_dbls_fpd_samps[i,:,0] = fpd_samps[:,0] - fpd_samps[:,1]\n",
    "    gold_dbls_gamma_samps[i,:] = lens_param_samps[:,3]\n",
    "\n",
    "# silver doubles\n",
    "silver_dbls_fpd_samps = np.zeros((len(silver_dbls),num_samps,3))\n",
    "silver_dbls_gamma_samps = np.empty((len(silver_dbls),num_samps))\n",
    "for i in range(0,len(silver_dbls)):\n",
    "    lens_param_samps = multivariate_normal.rvs(mean=hst_mu[silver_dbls[i]],\n",
    "        cov=hst_cov[silver_dbls[i]],size=num_samps)\n",
    "    x_im = gt_lenses.lens_df.loc[silver_dbls[i],['x_im0','x_im1']].to_numpy().astype(float)\n",
    "    y_im = gt_lenses.lens_df.loc[silver_dbls[i],['y_im0','y_im1']].to_numpy().astype(float)\n",
    "    fpd_samps = batched_fermatpot.eplshear_fp_samples(x_im,y_im,\n",
    "        lens_param_samps[:,:8],lens_param_samps[:,8],lens_param_samps[:,9])\n",
    "    silver_dbls_fpd_samps[i,:,0] = fpd_samps[:,0] - fpd_samps[:,1]\n",
    "    silver_dbls_gamma_samps[i,:] = lens_param_samps[:,3]\n",
    "\n",
    "\n",
    "# gold quads\n",
    "gold_quads_fpd_samps = np.zeros((len(gold_quads),num_samps,3))\n",
    "gold_quads_gamma_samps = np.empty((len(gold_quads),num_samps))\n",
    "gold_quads_fpd_samps_EM = np.zeros((len(gold_quads),num_samps,3))\n",
    "gold_quads_gamma_samps_EM = np.empty((len(gold_quads),num_samps))\n",
    "\n",
    "for i in range(0,len(gold_quads)):\n",
    "    lens_param_samps = multivariate_normal.rvs(mean=hst_mu[gold_quads[i]],\n",
    "        cov=hst_cov[gold_quads[i]],size=num_samps)\n",
    "    \n",
    "    # TODO: replace lens_params_samps for a gold_quads debugging test\n",
    "    mu_em = gt_lenses.lens_df.loc[gold_quads[i],['theta_E', 'gamma1', 'gamma2', 'gamma', 'e1', 'e2', \n",
    "            'center_x', 'center_y', 'src_center_x','src_center_y']].to_numpy().astype(float)\n",
    "    sigmas_em = np.asarray([0.01,0.01,0.01,0.06,0.02,0.02,0.005,0.005,0.005,0.005])\n",
    "    cov_em = np.diag(sigmas_em**2)\n",
    "    lens_params_samps_em = multivariate_normal.rvs(mean=mu_em,cov=cov_em,size=num_samps)\n",
    "\n",
    "    x_im = gt_lenses.lens_df.loc[gold_quads[i],['x_im0','x_im1','x_im2','x_im3']].to_numpy().astype(float)\n",
    "    y_im = gt_lenses.lens_df.loc[gold_quads[i],['y_im0','y_im1','y_im2','y_im3']].to_numpy().astype(float)\n",
    "\n",
    "    fpd_samps = batched_fermatpot.eplshear_fp_samples(x_im,y_im,\n",
    "        lens_param_samps[:,:8],lens_param_samps[:,8],lens_param_samps[:,9])\n",
    "    gold_quads_fpd_samps[i,:,0] = fpd_samps[:,0] - fpd_samps[:,1]\n",
    "    gold_quads_fpd_samps[i,:,1] = fpd_samps[:,0] - fpd_samps[:,2]\n",
    "    gold_quads_fpd_samps[i,:,2] = fpd_samps[:,0] - fpd_samps[:,3]\n",
    "    gold_quads_gamma_samps[i,:] = lens_param_samps[:,3]\n",
    "\n",
    "    fpd_samps_em = batched_fermatpot.eplshear_fp_samples(x_im,y_im,\n",
    "        lens_params_samps_em[:,:8],lens_params_samps_em[:,8],lens_params_samps_em[:,9])\n",
    "    gold_quads_fpd_samps_EM[i,:,0] = fpd_samps_em[:,0] - fpd_samps_em[:,1]\n",
    "    gold_quads_fpd_samps_EM[i,:,1] = fpd_samps_em[:,0] - fpd_samps_em[:,2]\n",
    "    gold_quads_fpd_samps_EM[i,:,2] = fpd_samps_em[:,0] - fpd_samps_em[:,3]\n",
    "    gold_quads_gamma_samps_EM[i,:] = lens_params_samps_em[:,3]\n",
    "    \n",
    "\n",
    "# silver quads\n",
    "silver_quads_fpd_samps = np.zeros((len(silver_quads),num_samps,3))\n",
    "silver_quads_gamma_samps = np.empty((len(silver_quads),num_samps))\n",
    "for i in range(0,len(silver_quads)):\n",
    "    lens_param_samps = multivariate_normal.rvs(mean=hst_mu[silver_quads[i]],\n",
    "        cov=hst_cov[silver_quads[i]],size=num_samps)\n",
    "    x_im = gt_lenses.lens_df.loc[silver_quads[i],['x_im0','x_im1','x_im2','x_im3']].to_numpy().astype(float)\n",
    "    y_im = gt_lenses.lens_df.loc[silver_quads[i],['y_im0','y_im1','y_im2','y_im3']].to_numpy().astype(float)\n",
    "    fpd_samps = batched_fermatpot.eplshear_fp_samples(x_im,y_im,\n",
    "        lens_param_samps[:,:8],lens_param_samps[:,8],lens_param_samps[:,9])\n",
    "    silver_quads_fpd_samps[i,:,0] = fpd_samps[:,0] - fpd_samps[:,1]\n",
    "    silver_quads_fpd_samps[i,:,1] = fpd_samps[:,0] - fpd_samps[:,2]\n",
    "    silver_quads_fpd_samps[i,:,2] = fpd_samps[:,0] - fpd_samps[:,3]\n",
    "    silver_quads_gamma_samps[i,:] = lens_param_samps[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all data vectors to an *informatively named* folder ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_folder_name = 'gold2days_silver5days'\n",
    "\n",
    "# let's dump everything into .h5 files\n",
    "\n",
    "h5f = h5py.File(('DataVectors/'+exp_folder_name+'/gold_quads.h5'), 'w')\n",
    "h5f.create_dataset('measured_td', data=gold_quads_measured_td)\n",
    "h5f.create_dataset('measured_prec',data=gold_quads_measured_prec)\n",
    "h5f.create_dataset('prefactor',data=gold_quads_prefactor)\n",
    "h5f.create_dataset('fpd_samps',data=gold_quads_fpd_samps)\n",
    "h5f.create_dataset('gamma_samps',data=gold_quads_gamma_samps)\n",
    "h5f.create_dataset('z_lens',data=gt_lenses.lens_df.loc[gold_quads,'z_lens'].to_numpy().astype(float))\n",
    "h5f.create_dataset('z_src',data=gt_lenses.lens_df.loc[gold_quads,'z_src'].to_numpy().astype(float))\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File(('DataVectors/'+exp_folder_name+'/gold_dbls.h5'), 'w')\n",
    "h5f.create_dataset('measured_td', data=gold_dbls_measured_td)\n",
    "h5f.create_dataset('measured_prec',data=gold_dbls_measured_prec)\n",
    "h5f.create_dataset('prefactor',data=gold_dbls_prefactor)\n",
    "h5f.create_dataset('fpd_samps',data=gold_dbls_fpd_samps)\n",
    "h5f.create_dataset('gamma_samps',data=gold_dbls_gamma_samps)\n",
    "h5f.create_dataset('z_lens',data=gt_lenses.lens_df.loc[gold_dbls,'z_lens'].to_numpy().astype(float))\n",
    "h5f.create_dataset('z_src',data=gt_lenses.lens_df.loc[gold_dbls,'z_src'].to_numpy().astype(float))\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File(('DataVectors/'+exp_folder_name+'/silver_quads.h5'), 'w')\n",
    "h5f.create_dataset('measured_td', data=silver_quads_measured_td)\n",
    "h5f.create_dataset('measured_prec',data=silver_quads_measured_prec)\n",
    "h5f.create_dataset('prefactor',data=silver_quads_prefactor)\n",
    "h5f.create_dataset('fpd_samps',data=silver_quads_fpd_samps)\n",
    "h5f.create_dataset('gamma_samps',data=silver_quads_gamma_samps)\n",
    "h5f.create_dataset('z_lens',data=gt_lenses.lens_df.loc[silver_quads,'z_lens'].to_numpy().astype(float))\n",
    "h5f.create_dataset('z_src',data=gt_lenses.lens_df.loc[silver_quads,'z_src'].to_numpy().astype(float))\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File(('DataVectors/'+exp_folder_name+'/silver_dbls.h5'), 'w')\n",
    "h5f.create_dataset('measured_td', data=silver_dbls_measured_td)\n",
    "h5f.create_dataset('measured_prec',data=silver_dbls_measured_prec)\n",
    "h5f.create_dataset('prefactor',data=silver_dbls_prefactor)\n",
    "h5f.create_dataset('fpd_samps',data=silver_dbls_fpd_samps)\n",
    "h5f.create_dataset('gamma_samps',data=silver_dbls_gamma_samps)\n",
    "h5f.create_dataset('z_lens',data=gt_lenses.lens_df.loc[silver_dbls,'z_lens'].to_numpy().astype(float))\n",
    "h5f.create_dataset('z_src',data=gt_lenses.lens_df.loc[silver_dbls,'z_src'].to_numpy().astype(float))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try to use TDC Sampler ###\n",
    "\n",
    "We'll just use the gold quads, since this is a small number of lenses, so the inference can run in a reasonable amount of time. These lenses are also the most constraining, which is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdc_sampler\n",
    "\n",
    "# let's just try with gold quads\n",
    "my_tdc = tdc_sampler.TDCLikelihood(gold_quads_measured_td,gold_quads_measured_prec,\n",
    "    gold_quads_prefactor,gold_quads_fpd_samps,gold_quads_gamma_samps,\n",
    "    gt_lenses.lens_df.loc[gold_quads,'z_lens'].to_numpy().astype(float),\n",
    "    gt_lenses.lens_df.loc[gold_quads,'z_src'].to_numpy().astype(float),\n",
    "    log_prob_modeling_prior='hardcoded')\n",
    "\n",
    "mcmc_chain_quads = tdc_sampler.fast_TDC(my_tdc,num_emcee_samps=1000,\n",
    "    cosmo_model='LCDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(gold_quads_measured_td[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0,wa Gold Quads\n",
    "# let's just try with gold quads\n",
    "my_tdc = tdc_sampler.TDCLikelihood(gold_quads_measured_td,gold_quads_measured_prec,\n",
    "    gold_quads_prefactor,gold_quads_fpd_samps,gold_quads_gamma_samps,\n",
    "    gt_lenses.lens_df.loc[gold_quads,'z_lens'].to_numpy().astype(float),\n",
    "    gt_lenses.lens_df.loc[gold_quads,'z_src'].to_numpy().astype(float),\n",
    "    log_prob_modeling_prior='hardcoded',cosmo_model='w0waCDM')\n",
    "\n",
    "mcmc_chain_quads_w0wa_1day = tdc_sampler.fast_TDC(my_tdc,cosmo_model='w0waCDM',\n",
    "    num_emcee_samps=6000,n_walkers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0,wa Gold Quads\n",
    "# let's just try with gold quads\n",
    "my_tdc = tdc_sampler.TDCLikelihood(gold_quads_measured_td,gold_quads_measured_prec,\n",
    "    gold_quads_prefactor,gold_quads_fpd_samps_EM,gold_quads_gamma_samps_EM,\n",
    "    gt_lenses.lens_df.loc[gold_quads,'z_lens'].to_numpy().astype(float),\n",
    "    gt_lenses.lens_df.loc[gold_quads,'z_src'].to_numpy().astype(float),\n",
    "    log_prob_modeling_prior='hardcoded',cosmo_model='w0waCDM')\n",
    "\n",
    "mcmc_chain_quads_w0wa_1day_EM = tdc_sampler.fast_TDC(my_tdc,cosmo_model='w0waCDM',\n",
    "    num_emcee_samps=6000,n_walkers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0,wa Gold Quads + Gold doubles\n",
    "\n",
    "gold_measured_td = np.vstack((\n",
    "    gold_quads_measured_td,\n",
    "    gold_dbls_measured_td))\n",
    "gold_measured_prec = np.vstack((\n",
    "    gold_quads_measured_prec,\n",
    "    gold_dbls_measured_prec\n",
    "))\n",
    "gold_prefactor = np.append(\n",
    "    gold_quads_prefactor,\n",
    "    gold_dbls_prefactor\n",
    ")\n",
    "gold_fpd_samps = np.vstack((\n",
    "    gold_quads_fpd_samps,\n",
    "    gold_dbls_fpd_samps\n",
    "))\n",
    "gold_gamma_samps = np.vstack((\n",
    "    gold_quads_gamma_samps,\n",
    "    gold_dbls_gamma_samps\n",
    "))\n",
    "\n",
    "gold_idxs = np.hstack((gold_quads,gold_dbls))\n",
    "\n",
    "\n",
    "\n",
    "# let's just try with gold quads\n",
    "my_tdc = tdc_sampler.TDCLikelihood(gold_measured_td,gold_measured_prec,\n",
    "    gold_prefactor,gold_fpd_samps,gold_gamma_samps,\n",
    "    gt_lenses.lens_df.loc[gold_idxs,'z_lens'].to_numpy().astype(float),\n",
    "    gt_lenses.lens_df.loc[gold_idxs,'z_src'].to_numpy().astype(float),\n",
    "    log_prob_modeling_prior='hardcoded',cosmo_model='w0waCDM')\n",
    "\n",
    "mcmc_chain_quads_w0wa_200 = tdc_sampler.fast_TDC(my_tdc,cosmo_model='w0waCDM',\n",
    "    num_emcee_samps=6000,n_walkers=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Chains for Gold Quads, LCDM vs. w0waCDM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean_quad = np.mean(gt_lenses.lens_df.loc[gold_quads,'gamma'].to_numpy().astype(float))\n",
    "std_mean_quad = np.std(gt_lenses.lens_df.loc[gold_quads,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "\n",
    "exp_chains = [mcmc_chain_quads]\n",
    "exp_names = ['ML Posteriors, Gold Quads']\n",
    "burnin = 400\n",
    "colors = ['gold']\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin:,:].reshape((-1,3)),plot_datapoints=False,\n",
    "                    color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "                    labels=['$H_0$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "                    dpi=200,truths=[70.,mu_mean_quad,std_mean_quad],truth_color='black',\n",
    "                    fig=None,label_kwargs={'fontsize':17})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin:,:].reshape((-1,3)),plot_datapoints=False,\n",
    "                    color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "                    labels=['$H_0$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "                    truths=[70.,mu_mean_quad,std_mean_quad],truth_color='black',\n",
    "                    fig=figure,label_kwargs={'fontsize':17})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "axes[0,2].legend(custom_lines,custom_labels,frameon=False,fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(silver,gold).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('DataVectors/gold2days_silver5days/gold_chain.h5', 'r')\n",
    "gold_chain = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('DataVectors/gold2days_silver5days/silver_chain.h5', 'r')\n",
    "silver_chain = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('DataVectors/gold2days_silver5days/gold_chain1000.h5', 'r')\n",
    "gold_chain1000 = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float))\n",
    "std_mean_gold = np.std(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "mu_mean_silver = np.mean(gt_lenses.lens_df.loc[np.append(silver,gold),'gamma'].to_numpy().astype(float))\n",
    "std_mean_silver = np.std(gt_lenses.lens_df.loc[np.append(silver,gold),'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "exp_chains = [gold_chain,gold_chain1000]#mcmc_chain_quads_w0wa, mcmc_chain_quads_w0wa_1day,mcmc_chain_quads_w0wa_1day_EM, mcmc_chain_quads_w0wa_200]\n",
    "exp_names = ['Gold: 50 Quads+ 200 Dbls, NPE Models, 5e3 fpd samps', #2-day td error\n",
    "             'Gold: 50 Quads+ 200 Dbls, NPE Models, 1e3 fpd samps',\n",
    "             '250Gold+1034Silver, NPE Models, 5-day silver TD error']#'50 Gold Quads, NPE Models, 2-day td error', \n",
    "             #'50 Gold Quads, NPE Models, 1-day td error', \n",
    "             #'50 Gold Quads, Emulated Models, 1-day td error',\n",
    "             #'Gold: 50 Quads+ 200 Dbls, NPE Models, 1-day td error']\n",
    "burnin = int(2e3)\n",
    "colors = ['purple','turquoise']#'gold','silver','indianred','turquoise','purple']\n",
    "custom_labels = []\n",
    "\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin:,:].reshape((-1,5)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,-1.,0.,mu_mean_gold,std_mean_gold],truth_color='black',\n",
    "            fig=None,label_kwargs={'fontsize':17})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin:,:].reshape((-1,5)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,-1.,0.,mu_mean_silver,std_mean_silver],truth_color='black',\n",
    "            fig=figure,label_kwargs={'fontsize':17})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((5, 5))\n",
    "axes[0,4].legend(custom_lines,custom_labels,frameon=False,fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gt_lenses.lens_df.loc[np.append(silver,gold),'gamma'].to_numpy().astype(float),bins=20,density=True)\n",
    "my_x = np.arange(1.55,2.6,0.01)\n",
    "from scipy.stats import norm\n",
    "plt.plot(my_x,norm.pdf(my_x,loc=mu_mean_silver,scale=std_mean_silver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_labels = np.asarray(['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'])\n",
    "true_hyperparameters = np.asarray([70.,-1.,0.,mu_mean_quad,std_mean_quad])\n",
    "\n",
    "mcmc_utils.analyze_chains(gold_chain,param_labels,true_hyperparameters,\n",
    "                    'test.txt',show_chains=True,\n",
    "                    burnin=int(1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDC Sampler Gold Doubles & Quads!! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just try with gold doubles\n",
    "my_tdc = tdc_sampler.TDCLikelihood(gold_dbls_measured_td,gold_dbls_measured_prec,\n",
    "    gold_dbls_prefactor,gold_dbls_fpd_samps,gold_dbls_gamma_samps,\n",
    "    gt_lenses.lens_df.loc[gold_dbls,'z_lens'].to_numpy().astype(float),\n",
    "    gt_lenses.lens_df.loc[gold_dbls,'z_src'].to_numpy().astype(float),\n",
    "    log_prob_modeling_prior='hardcoded')\n",
    "\n",
    "mcmc_chain_dbls = tdc_sampler.fast_TDC(my_tdc,num_emcee_samps=1000,\n",
    "    cosmo_model='LCDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just try with gold quads\n",
    "\n",
    "gold_idxs = np.append(gold_quads,gold_dbls)\n",
    "\n",
    "my_tdc = tdc_sampler.TDCLikelihood(\n",
    "    np.vstack((gold_quads_measured_td,gold_dbls_measured_td)),\n",
    "    np.vstack((gold_quads_measured_prec,gold_dbls_measured_prec)),\n",
    "    np.append(gold_quads_prefactor,gold_dbls_prefactor),\n",
    "    np.vstack((gold_quads_fpd_samps,gold_dbls_fpd_samps)),\n",
    "    np.vstack((gold_quads_gamma_samps,gold_dbls_gamma_samps)),\n",
    "    gt_lenses.lens_df.loc[gold_idxs,'z_lens'].to_numpy().astype(float),\n",
    "    gt_lenses.lens_df.loc[gold_idxs,'z_src'].to_numpy().astype(float),\n",
    "    log_prob_modeling_prior='hardcoded')\n",
    "\n",
    "mcmc_chain_comb = tdc_sampler.fast_TDC(my_tdc,num_emcee_samps=1000,\n",
    "    cosmo_model='LCDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean_quad = np.mean(gt_lenses.lens_df.loc[gold_quads,'gamma'].to_numpy().astype(float))\n",
    "mu_mean_double = np.mean(gt_lenses.lens_df.loc[gold_dbls,'gamma'].to_numpy().astype(float))\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold_idxs,'gamma'].to_numpy().astype(float))\n",
    "\n",
    "exp_chains = [mcmc_chain_quads,mcmc_chain_EM]\n",
    "exp_names = ['ML Posteriors, Gold Quads','Emulated Posteriors, Gold Quads']\n",
    "burnin = 200\n",
    "colors = ['gold','skyblue','silver','skyblue']\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin:,:].reshape((-1,3)),plot_datapoints=False,\n",
    "                    color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "                    labels=['$H_0$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "                    dpi=200,truths=[70.,mu_mean_quad,-1],truth_color='black',\n",
    "                    fig=None,label_kwargs={'fontsize':17})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin:,:].reshape((-1,3)),plot_datapoints=False,\n",
    "                    color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "                    labels=['$H_0$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "                    truths=[70.,mu_mean_quad,-1],truth_color='black',\n",
    "                    fig=figure,label_kwargs={'fontsize':17})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "axes[0,2].legend(custom_lines,custom_labels,frameon=False,fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean_quad = np.mean(gt_lenses.lens_df.loc[gold_quads,'gamma'].to_numpy().astype(float))\n",
    "mu_mean_double = np.mean(gt_lenses.lens_df.loc[gold_dbls,'gamma'].to_numpy().astype(float))\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold_idxs,'gamma'].to_numpy().astype(float))\n",
    "\n",
    "exp_chains = [mcmc_chain_quads,mcmc_chain_dbls,mcmc_chain_comb]\n",
    "exp_names = ['Gold Quads','Gold Doubles','Gold Quads+Doubles']\n",
    "burnin = 400\n",
    "colors = ['gold','skyblue','salmon','skyblue']\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin:,:].reshape((-1,3)),plot_datapoints=False,\n",
    "                    color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "                    labels=['$H_0$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "                    dpi=200,truths=[70.,-1,-1],truth_color='black',\n",
    "                    fig=None,label_kwargs={'fontsize':17})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin:,:].reshape((-1,3)),plot_datapoints=False,\n",
    "                    color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "                    labels=['$H_0$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "                    truths=[70.,-1,-1],truth_color='black',\n",
    "                    fig=figure,label_kwargs={'fontsize':17})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "axes[0,2].legend(custom_lines,custom_labels,frameon=False,fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Look at w0waCDM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold_idxs,'gamma'].to_numpy().astype(float))\n",
    "\n",
    "h5f = h5py.File('new_mass_models/gold_chain.h5', 'r')\n",
    "gold_chain = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('new_mass_models/silver_chain.h5', 'r')\n",
    "silver_chain = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "exp_chains = [gold_chain, silver_chain]\n",
    "exp_names = ['Gold, 70 Lenses', 'Gold+Silver, 1284 Lenses ']\n",
    "burnin = 400\n",
    "colors = ['gold','skyblue','salmon','skyblue']\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin:,:].reshape((-1,5)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,-1.,0.,-1,-1],truth_color='black',\n",
    "            fig=None,label_kwargs={'fontsize':17})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin:,:].reshape((-1,5)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,-1.,0.,-1,-1],truth_color='black',\n",
    "            fig=figure,label_kwargs={'fontsize':17})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((5, 5))\n",
    "axes[0,4].legend(custom_lines,custom_labels,frameon=False,fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the assumed training prior match the effective training prior after cuts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_val = pd.read_csv('MassModels/hst_validation_metadata.csv')\n",
    "gamma_vals = hst_val['main_deflector_parameters_gamma'].to_numpy().astype(float)\n",
    "plt.hist(gamma_vals,density=True,label='Samples')\n",
    "x_range = np.arange(1.4,2.6,0.01)\n",
    "mu_est = np.mean(gamma_vals)\n",
    "sigma_est = np.std(gamma_vals,ddof=1)\n",
    "plt.plot(x_range,norm.pdf(x_range,mu_est,sigma_est),label='$\\mu=%.3f$, $\\sigma=%.2f$'%(mu_est,sigma_est))\n",
    "plt.legend()\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.figure()\n",
    "hst_train0 = pd.read_csv('MassModels/hst_train0_metadata.csv')\n",
    "gamma_vals = hst_train0['main_deflector_parameters_gamma'].to_numpy().astype(float)\n",
    "plt.hist(gamma_vals,density=True,label='Samples')\n",
    "x_range = np.arange(1.4,2.6,0.01)\n",
    "mu_est = np.mean(gamma_vals)\n",
    "sigma_est = np.std(gamma_vals,ddof=1)\n",
    "plt.plot(x_range,norm.pdf(x_range,mu_est,sigma_est),label='$\\mu=%.3f$, $\\sigma=%.2f$'%(mu_est,sigma_est))\n",
    "plt.legend()\n",
    "plt.title('Train 0')\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import gaussian_kde\n",
    "plt.figure()\n",
    "\n",
    "gaussian_samples = norm.rvs(mu_est,sigma_est,size=10000)\n",
    "\n",
    "# Let's try a KDE\n",
    "gamma_kde = gaussian_kde(gamma_vals)\n",
    "kde_samples = gamma_kde.resample(size=10000)[0]\n",
    "bins=np.histogram(np.hstack((kde_samples,gamma_vals)), bins=40)[1]\n",
    "\n",
    "counts_exp,_,_ = plt.hist(kde_samples,bins,\n",
    "            histtype='step',label='KDE Estimate')\n",
    "counts_obs,_,_ = plt.hist(gamma_vals,bins,histtype='step',\n",
    "                          label='Training Samples')\n",
    "    \n",
    "# only take bins where counts are greater than 5, then add in an extra bin at beginning and end for the tails\n",
    "idx = np.where((counts_exp > 5) & (counts_obs > 5))[0]\n",
    "# less than some #, bins, greather than some #\n",
    "prepend = np.sum(counts_obs[:idx[0]])\n",
    "append = np.sum(counts_obs[(idx[-1]+1):])\n",
    "counts_obs_final = np.concatenate(([prepend],counts_obs[idx],[append]))\n",
    "prepend = np.sum(counts_exp[:idx[0]])\n",
    "append = np.sum(counts_exp[(idx[-1]+1):])\n",
    "counts_exp_final = np.concatenate(([prepend],counts_exp[idx],[append]))\n",
    "\n",
    "\n",
    "\n",
    "chi2_distance = np.sum((counts_obs_final-counts_exp_final)**2/counts_exp_final)\n",
    "#print(chi2_distance)\n",
    "\n",
    "chi2,_= chisquare(counts_obs_final,counts_exp_final)\n",
    "print(chi2)\n",
    "\n",
    "dof = len(counts_obs_final) - 1\n",
    "print('chi2/dof:', chi2/dof)\n",
    "plt.text(1.6,800,r'$\\frac{\\chi^2}{\\nu}$: %.2f'%(chi2/dof),\n",
    "                        {'fontsize':13})\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emcee_samps = 6000\n",
    "exp_folder = 'gold2days_silver5days'\n",
    "\n",
    "lens_types = ['gold_quads','gold_dbls','silver_quads','silver_dbls']\n",
    "\n",
    "inputs_dict = {\n",
    "    'gold_quads':{},\n",
    "    'gold_dbls':{},\n",
    "    'silver_quads':{},\n",
    "    'silver_dbls':{}\n",
    "}\n",
    "\n",
    "input_keys = ['measured_td','measured_prec','prefactor','fpd_samps','gamma_samps','z_lens','z_src']\n",
    "\n",
    "for l in lens_types:\n",
    "\n",
    "    my_filepath = ('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/'+\n",
    "        'DataVectors/'+exp_folder+'/'+l+'.h5')\n",
    "    h5f = h5py.File(my_filepath, 'r')\n",
    "    for key in input_keys:\n",
    "        inputs_dict[l][key] = h5f.get(key)[:]\n",
    "    h5f.close()\n",
    "\n",
    "\n",
    "# let's run gold-only and gold+silver\n",
    "gold_measured_td = np.vstack((\n",
    "    inputs_dict['gold_quads']['measured_td'],\n",
    "    inputs_dict['gold_dbls']['measured_td']))\n",
    "gold_measured_prec = np.vstack((\n",
    "    inputs_dict['gold_quads']['measured_prec'],\n",
    "    inputs_dict['gold_dbls']['measured_prec']\n",
    "))\n",
    "gold_prefactor = np.append(\n",
    "    inputs_dict['gold_quads']['prefactor'],\n",
    "    inputs_dict['gold_dbls']['prefactor']\n",
    ")\n",
    "# TODO: try only 1,000 samps here\n",
    "gold_fpd_samps = np.vstack((\n",
    "    inputs_dict['gold_quads']['fpd_samps'],\n",
    "    inputs_dict['gold_dbls']['fpd_samps']\n",
    "))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dict['gold_quads']['prefactor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dict['gold_dbls']['fpd_samps'][:,:1000,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

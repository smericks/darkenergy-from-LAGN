{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lens_catalog import OM10LensCatalog\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils.inference_utils import median_sigma_from_samples\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import corner\n",
    "from matplotlib.lines import Line2D\n",
    "import Utils.mcmc_utils as mcmc_utils\n",
    "import tdc_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lenses = OM10LensCatalog('MassModels/om10_sample/om10_venkatraman_erickson24.csv')\n",
    "gt_lenses.lens_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Gold/Silver Samples ###\n",
    "\n",
    "We split the sample into doubles and quads. We already have a sample of ~30 quads (the STRIDES sample), so we take a conservative assumption of 50 quads in the gold sample. We add 200 doubles to the gold sample.\n",
    "The remaining lenses are added to the silver sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into doubles & quads\n",
    "dbls = gt_lenses.doubles_indices()\n",
    "quads = gt_lenses.quads_indices()\n",
    "\n",
    "# require mag_app_src < 24\n",
    "mag_limit = True\n",
    "if mag_limit:\n",
    "    # NOTE: hardcoded mag limit!!\n",
    "    src_mag_app_lim = 24.\n",
    "    bright_idxs = np.where(gt_lenses.lens_df['src_mag_app'] < src_mag_app_lim)[0]\n",
    "\n",
    "    # find the overlap\n",
    "    dbls = np.intersect1d(dbls, bright_idxs)\n",
    "    quads = np.intersect1d(quads,bright_idxs)\n",
    "\n",
    "# Let's assume in the gold sample: 200 doubles, 50 quads (overall quad fraction is 11%, so this is amplified)\n",
    "# The rest are silver (regardless of time delay)\n",
    "gold_dbls = dbls[-200:]#dbls[:200]\n",
    "silver_dbls = dbls[:-200]#dbls[200:]\n",
    "\n",
    "gold_quads = quads[-50:]#quads[:50]\n",
    "silver_quads = quads[:-50]#quads[50:]\n",
    "\n",
    "gold = np.append(gold_dbls,gold_quads)\n",
    "silver = np.append(silver_dbls,silver_quads)\n",
    "\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float))\n",
    "std_mean_gold = np.std(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "print('mean gamma of gold sample: ', mu_mean_gold)\n",
    "print('sigma gamma of gold sample: ', std_mean_gold)\n",
    "\n",
    "mu_mean_silver = np.mean(gt_lenses.lens_df.loc[silver,'gamma'].to_numpy().astype(float))\n",
    "std_mean_silver = np.std(gt_lenses.lens_df.loc[silver,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "print('mean gamma of silver sample: ', mu_mean_silver)\n",
    "print('sigma gamma of silver sample: ', std_mean_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Data Vectors ###\n",
    "\n",
    "This is all done using the make_data_vectors() function in make_data_vectors.py\n",
    "\n",
    "1. Emulates time delay measurements\n",
    "2. Uses NPE posteriors to compute samples from fermat potential difference posteriors\n",
    "3. Stores all information in format for fast_tdc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_data_vectors import make_data_vectors\n",
    "\n",
    "# load in NPE posteriors\n",
    "hst_mu = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/y_pred_hst.npy')\n",
    "hst_cov = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/cov_pred_hst.npy')\n",
    "\n",
    "lsst_mu = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/y_pred_lsst_DEBIASED2.npy')\n",
    "lsst_cov = np.load('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/MassModels/om10_sample/cov_pred_lsst_DEBIASED2.npy')\n",
    "\n",
    "# where to store this test\n",
    "exp_folder = 'fpd_eval_DV_silverALLDEBIASED'\n",
    "\n",
    "# gold quads\n",
    "make_data_vectors(gt_lenses,gold_quads,num_images=4,td_meas_error=2,\n",
    "    npe_mu=hst_mu,npe_cov=hst_cov,\n",
    "    h5_save_path=('DataVectors/'+exp_folder+'/gold_quads.h5'),\n",
    "    num_fpd_samps=3000,emulated=False)\n",
    "\n",
    "# gold dbls\n",
    "make_data_vectors(gt_lenses,gold_dbls,num_images=2,td_meas_error=2,\n",
    "    npe_mu=hst_mu,npe_cov=hst_cov,\n",
    "    h5_save_path=('DataVectors/'+exp_folder+'/gold_dbls.h5'),\n",
    "    num_fpd_samps=3000,emulated=False)\n",
    "\n",
    "# silver quads\n",
    "make_data_vectors(gt_lenses,silver_quads,num_images=4,td_meas_error=5,\n",
    "    npe_mu=lsst_mu,npe_cov=lsst_cov,\n",
    "    h5_save_path=('DataVectors/'+exp_folder+'/silver_quads.h5'),\n",
    "    num_fpd_samps=3000,emulated=False)\n",
    "\n",
    "# silver dbls\n",
    "make_data_vectors(gt_lenses,silver_dbls,num_images=2,td_meas_error=5,\n",
    "    npe_mu=lsst_mu,npe_cov=lsst_cov,\n",
    "    h5_save_path=('DataVectors/'+exp_folder+'/silver_dbls.h5'),\n",
    "    num_fpd_samps=3000,emulated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check fermat potential bias ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where data vectors are stored\n",
    "exp_folder = ('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/'+\n",
    "        'DataVectors/fpd_eval_DV_silverALLDEBIASED')\n",
    "\n",
    "lens_types = ['gold_quads','gold_dbls']\n",
    "\n",
    "inputs_dict = {\n",
    "    'gold_quads':{},\n",
    "    'gold_dbls':{},\n",
    "}\n",
    "\n",
    "input_keys = ['measured_td','measured_prec','prefactor','fpd_samps','gamma_samps','z_lens_truth','z_src_truth']\n",
    "\n",
    "for l in lens_types:\n",
    "    my_filepath = (exp_folder+'/'+l+'.h5')\n",
    "    h5f = h5py.File(my_filepath, 'r')\n",
    "    for key in input_keys:\n",
    "        inputs_dict[l][key] = h5f.get(key)[:]\n",
    "    h5f.close()\n",
    "\n",
    "quads_fpd_samps = inputs_dict['gold_quads']['fpd_samps']\n",
    "quads_median_fpd = np.median(quads_fpd_samps,axis=1)\n",
    "low = np.quantile(quads_fpd_samps,q=0.1586,axis=1)\n",
    "high = np.quantile(quads_fpd_samps,q=0.8413,axis=1)\n",
    "quads_sigma_fpd = ((high-quads_median_fpd)+(quads_median_fpd-low))/2\n",
    "\n",
    "dbls_fpd_samps = inputs_dict['gold_dbls']['fpd_samps']\n",
    "dbls_median_fpd = np.median(dbls_fpd_samps,axis=1)\n",
    "low = np.quantile(dbls_fpd_samps,q=0.1586,axis=1)\n",
    "high = np.quantile(dbls_fpd_samps,q=0.8413,axis=1)\n",
    "dbls_sigma_fpd = ((high-dbls_median_fpd)+(dbls_median_fpd-low))/2\n",
    "\n",
    "#  QUADS\n",
    "fig,axs = plt.subplots(1,3,figsize=(13,4),dpi=200)\n",
    "# iterate over fpds\n",
    "for i in range(0,3):\n",
    "    true_fpd = gt_lenses.lens_df.loc[gold_quads,'fpd0%d'%(i+1)].to_numpy().astype(float)\n",
    "    pred_fpd = quads_median_fpd[:,i]\n",
    "    sigma_pred_fpd = quads_sigma_fpd[:,i]\n",
    "    axs[i].plot([-1.9,0.2],[-1.9,0.2],color='black')\n",
    "    axs[i].scatter(true_fpd,pred_fpd,color='goldenrod')\n",
    "    axs[i].set_title('fpd0%d'%(i+1))\n",
    "    axs[i].set_xlabel('truth fpd0%d'%(i+1))\n",
    "    axs[i].set_ylabel('pred fpd0%d'%(i+1))\n",
    "    mean_error = np.mean(pred_fpd - true_fpd)\n",
    "    mean_error_sigma = np.mean( (pred_fpd - true_fpd) / quads_sigma_fpd[:,i])\n",
    "    axs[i].text(-0.9,-1.3,'mean_error = %.3f'%(mean_error))\n",
    "    axs[i].text(-0.9,-1.5,'mean_error / $\\sigma$ = %.2f'%(mean_error_sigma))\n",
    "\n",
    "plt.suptitle('Gold Quads fpd recovery')\n",
    "\n",
    "# DOUBLES\n",
    "plt.figure(dpi=200)\n",
    "i=0\n",
    "true_fpd = gt_lenses.lens_df.loc[gold_dbls,'fpd0%d'%(i+1)].to_numpy().astype(float)\n",
    "pred_fpd = dbls_median_fpd[:,i]\n",
    "plt.plot([-4.,0.2],[-4.,0.2],color='black')\n",
    "plt.scatter(true_fpd,pred_fpd,color='goldenrod')\n",
    "plt.title('Gold Doubles fpd recovery')\n",
    "plt.xlabel('truth fpd0%d'%(i+1))\n",
    "plt.ylabel('pred fpd0%d'%(i+1))\n",
    "mean_error = np.mean(pred_fpd - true_fpd)\n",
    "mean_error_sigma = np.mean( (pred_fpd - true_fpd) / dbls_sigma_fpd[:,i])\n",
    "plt.text(-1.5,-3.5,'mean_error = %.3f'%(mean_error))\n",
    "plt.text(-1.5,-4.,'mean_error / $\\sigma$ = %.2f'%(mean_error_sigma))\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "true_td = gt_lenses.lens_df.loc[gold_dbls,'td0%d'%(i+1)].to_numpy().astype(float)\n",
    "plt.errorbar(true_td, pred_fpd - true_fpd, yerr=dbls_sigma_fpd[:, i], fmt='o', color='goldenrod', markersize=5, elinewidth=1)\n",
    "plt.xlabel('td01 (days)')\n",
    "plt.ylabel('pred_fpd01 - true_fpd01')\n",
    "plt.hlines(0.,xmin=-850.,xmax=1.,color='black',zorder=200)\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.title('Gold Doubles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where data vectors are stored\n",
    "exp_folder = ('/Users/smericks/Desktop/StrongLensing/darkenergy-from-LAGN/'+\n",
    "        'DataVectors/fpd_eval_DV_silverALLDEBIASED')\n",
    "\n",
    "lens_types = ['gold_quads','gold_dbls','silver_quads','silver_dbls']\n",
    "\n",
    "inputs_dict = {\n",
    "    'gold_quads':{},\n",
    "    'gold_dbls':{},\n",
    "    'silver_quads':{},\n",
    "    'silver_dbls':{},\n",
    "}\n",
    "\n",
    "input_keys = ['measured_td','measured_prec','prefactor','fpd_samps','gamma_samps',\n",
    "    'z_lens_truth','z_src_truth','theta_E_truth','Ddt_Mpc_truth_truth',\n",
    "    'td01_truth','fpd01_truth','gamma_truth']\n",
    "\n",
    "for l in lens_types:\n",
    "    my_filepath = (exp_folder+'/'+l+'.h5')\n",
    "    h5f = h5py.File(my_filepath, 'r')\n",
    "    for key in input_keys:\n",
    "        inputs_dict[l][key] = h5f.get(key)[:]\n",
    "    h5f.close()\n",
    "\n",
    "quads_fpd_samps = inputs_dict['silver_quads']['fpd_samps']\n",
    "quads_median_fpd = np.median(quads_fpd_samps,axis=1)\n",
    "low = np.quantile(quads_fpd_samps,q=0.1586,axis=1)\n",
    "high = np.quantile(quads_fpd_samps,q=0.8413,axis=1)\n",
    "quads_sigma_fpd = ((high-quads_median_fpd)+(quads_median_fpd-low))/2\n",
    "\n",
    "dbls_fpd_samps = inputs_dict['silver_dbls']['fpd_samps']\n",
    "dbls_median_fpd = np.median(dbls_fpd_samps,axis=1)\n",
    "low = np.quantile(dbls_fpd_samps,q=0.1586,axis=1)\n",
    "high = np.quantile(dbls_fpd_samps,q=0.8413,axis=1)\n",
    "dbls_sigma_fpd = ((high-dbls_median_fpd)+(dbls_median_fpd-low))/2\n",
    "\n",
    "\n",
    "dbls_gamma_samps = inputs_dict['silver_dbls']['gamma_samps']\n",
    "dbls_median_gamma = np.median(dbls_gamma_samps,axis=1)\n",
    "low = np.quantile(dbls_gamma_samps,q=0.1586,axis=1)\n",
    "high = np.quantile(dbls_gamma_samps,q=0.8413,axis=1)\n",
    "dbls_sigma_gamma = ((high-dbls_median_gamma)+(dbls_median_gamma-low))/2\n",
    "\n",
    "#  QUADS\n",
    "fig,axs = plt.subplots(1,3,figsize=(13,4),dpi=200)\n",
    "# iterate over fpds\n",
    "for i in range(0,3):\n",
    "    true_fpd = gt_lenses.lens_df.loc[silver_quads,'fpd0%d'%(i+1)].to_numpy().astype(float)\n",
    "    pred_fpd = quads_median_fpd[:,i]\n",
    "    axs[i].plot([-1.5,0.2],[-1.5,0.2],color='black')\n",
    "    axs[i].scatter(true_fpd,pred_fpd,color='silver')\n",
    "    axs[i].set_title('fpd0%d'%(i+1))\n",
    "    axs[i].set_xlabel('truth fpd0%d'%(i+1))\n",
    "    axs[i].set_ylabel('pred fpd0%d'%(i+1))\n",
    "    mean_error = np.mean(pred_fpd - true_fpd)\n",
    "    mean_error_sigma = np.mean( (pred_fpd - true_fpd) / quads_sigma_fpd[:,i])\n",
    "    axs[i].text(-0.9,-1.3,'mean_error = %.3f'%(mean_error))\n",
    "    axs[i].text(-0.9,-1.5,'mean_error / $\\sigma$ = %.2f'%(mean_error_sigma))\n",
    "\n",
    "plt.suptitle('Silver Quads fpd recovery')\n",
    "\n",
    "# DOUBLES\n",
    "plt.figure(dpi=200)\n",
    "i=0\n",
    "true_fpd = inputs_dict['silver_dbls']['fpd01_truth']#gt_lenses.lens_df.loc[silver_dbls,'fpd0%d'%(i+1)].to_numpy().astype(float)\n",
    "pred_fpd = dbls_median_fpd[:,i]\n",
    "print(true_fpd.shape)\n",
    "print(pred_fpd.shape)\n",
    "plt.plot([-5.,0.2],[-5.,0.2],color='black')\n",
    "plt.scatter(true_fpd,pred_fpd,color='silver')\n",
    "plt.title('Silver Doubles fpd recovery')\n",
    "plt.xlabel('truth fpd0%d'%(i+1))\n",
    "plt.ylabel('pred fpd0%d'%(i+1))\n",
    "mean_error = np.mean(pred_fpd - true_fpd)\n",
    "mean_error_sigma = np.mean( (pred_fpd - true_fpd) / dbls_sigma_fpd[:,i])\n",
    "plt.text(-1.5,-3.5,'mean_error = %.3f'%(mean_error))\n",
    "plt.text(-1.5,-4.,'mean_error / $\\sigma$ = %.2f'%(mean_error_sigma))\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "true_td = inputs_dict['silver_dbls']['td01_truth']\n",
    "plt.errorbar(true_td, pred_fpd - true_fpd, yerr=dbls_sigma_fpd[:, i], fmt='o', color='silver', markersize=5, elinewidth=1)\n",
    "plt.xlabel('td01 (days)')\n",
    "plt.ylabel('pred_fpd01 - true_fpd01')\n",
    "plt.hlines(0.,xmin=-850.,xmax=1.,color='black',zorder=200)\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.title('Silver Doubles')\n",
    "\n",
    "# try with gamma_lens as well\n",
    "plt.figure(dpi=200)\n",
    "true_td = inputs_dict['silver_dbls']['td01_truth']\n",
    "true_gamma = inputs_dict['silver_dbls']['gamma_truth']\n",
    "plt.errorbar(inputs_dict['silver_dbls']['theta_E_truth'], pred_fpd - true_fpd, yerr=dbls_sigma_fpd[:, i], fmt='o', color='silver', markersize=5, elinewidth=1)\n",
    "plt.xlabel('theta_E')\n",
    "plt.ylabel('pred_fpd01 - true_fpd01')\n",
    "plt.hlines(0.,xmin=0.4,xmax=2.0,color='black',zorder=200)\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.title('Silver Doubles')\n",
    "true_theta_E = inputs_dict['silver_dbls']['theta_E_truth']\n",
    "print('mean error on gamma silver doubles, theta_E>1.2: ', np.mean((dbls_median_gamma-true_gamma)[true_theta_E>1.2]))\n",
    "print('median error on gamma silver doubles, theta_E>1.2: ', np.median((dbls_median_gamma-true_gamma)[true_theta_E>1.2]))\n",
    "\n",
    "print('mean error on gamma silver doubles, theta_E<1.2: ', np.mean((dbls_median_gamma-true_gamma)[true_theta_E<1.2]))\n",
    "print('median error on gamma silver doubles, theta_E<1.2: ', np.median((dbls_median_gamma-true_gamma)[true_theta_E<1.2]))\n",
    "\n",
    "\n",
    "# now let's try err(DDt) vs redshift\n",
    "true_Ddt = inputs_dict['silver_dbls']['Ddt_Mpc_truth_truth']\n",
    "pred_Ddt = tdc_utils.ddt_from_td_fpd(inputs_dict['silver_dbls']['measured_td'][:,i],pred_fpd)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.scatter(inputs_dict['silver_dbls']['z_lens_truth'],100*(pred_Ddt-true_Ddt)/true_Ddt,color='silver')\n",
    "plt.xlabel('z_lens',fontsize=15)\n",
    "plt.ylabel('Ddt \\% error (pred-truth)/truth',fontsize=15)\n",
    "plt.hlines(0.,0.,2.,color='black')\n",
    "plt.title('Silver Doubles, NPE-Debiased',fontsize=15)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.scatter(inputs_dict['silver_dbls']['z_src_truth'],100*(pred_Ddt-true_Ddt)/true_Ddt,color='silver')\n",
    "plt.xlabel('z_src',fontsize=15)\n",
    "plt.ylabel('Ddt \\% error (pred-truth)/truth',fontsize=15)\n",
    "plt.hlines(0.,0.,3.,color='black')\n",
    "plt.title('Silver Doubles, NPE-Debiased',fontsize=15)\n",
    "\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.scatter(np.abs(inputs_dict['silver_dbls']['td01_truth']),100*(pred_Ddt-true_Ddt)/true_Ddt,color='silver')\n",
    "plt.xlabel('$\\Delta t_{01}$ truth',fontsize=15)\n",
    "plt.ylabel('Ddt \\% error (pred-truth)/truth',fontsize=15)\n",
    "plt.hlines(0.,0.,850.,color='black')\n",
    "plt.title('Silver Doubles, NPE-Debiased',fontsize=15)\n",
    "\n",
    "print(np.where(np.abs(100*(pred_Ddt-true_Ddt)/true_Ddt > 100.))[0])\n",
    "bad_idx = [  4,  54,  87,  90 ,197 ,298, 324]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check chains from make_data_vectors() test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float))\n",
    "std_mean_gold = np.std(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "print('mean gamma of gold sample: ', mu_mean_gold)\n",
    "print('sigma gamma of gold sample: ', std_mean_gold)\n",
    "\n",
    "mu_mean_silver = np.mean(gt_lenses.lens_df.loc[silver,'gamma'].to_numpy().astype(float))\n",
    "std_mean_silver = np.std(gt_lenses.lens_df.loc[silver,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "print('mean gamma of silver sample: ', mu_mean_silver)\n",
    "print('sigma gamma of silver sample: ', std_mean_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('DataVectors/fpd_eval_DV_silverALLDEBIASED/silver_doubles_ALL_chain_5e3_w0waCDM.h5', 'r')\n",
    "silver_doublesALL = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('DataVectors/fpd_eval_DV_silverALLDEBIASED/silver_doubles_REMOVEBAD_chain_5e3_w0waCDM.h5', 'r')\n",
    "silver_doublesGOOD = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float))\n",
    "std_mean_gold = np.std(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "mu_mean_silver = np.mean(gt_lenses.lens_df.loc[silver,'gamma'].to_numpy().astype(float))\n",
    "std_mean_silver = np.std(gt_lenses.lens_df.loc[silver,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "mu_mean_silver_dbls = np.mean(gt_lenses.lens_df.loc[silver_dbls,'gamma'].to_numpy().astype(float))\n",
    "std_mean_silver_dbls = np.std(gt_lenses.lens_df.loc[silver_dbls,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "exp_chains = [silver_doublesALL,silver_doublesGOOD]\n",
    "exp_names = ['All Silver Doubles: LSST NPE, m_app_src < 24',\n",
    "    'Bad Removed Silver Doubles: LSST NPE, m_app_src < 24',\n",
    "             'Combined: m_app_src < 24']\n",
    "burnin = [int(3e3),int(3e3),int(3e3),int(3e3),int(3e3)]\n",
    "colors = ['indianred','darkcyan','silver','indianred','darkturquoise','lightcyan','darkcyan','darkturquoise']#'indianred','gold','silver','indianred','turquoise','purple']\n",
    "custom_labels = []\n",
    "\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin[i]:,:].reshape((-1,6)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','$\\Omega_M$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,0.3,-1.,0.,mu_mean_silver_dbls,std_mean_silver_dbls],truth_color='black',\n",
    "            fig=None,label_kwargs={'fontsize':30},smooth=0.7,hist_kwargs={'density':True})\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin[i]:,:].reshape((-1,6)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','$\\Omega_M$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=200,truths=[70.,0.3,-1.,0.,mu_mean_silver_dbls,std_mean_silver_dbls],truth_color='black',\n",
    "            fig=figure,label_kwargs={'fontsize':30},smooth=0.7,hist_kwargs={'density':True})\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin[i]:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((6, 6))\n",
    "axes[0,5].legend(custom_lines,custom_labels,frameon=False,fontsize=20)\n",
    "\n",
    "plt.savefig('/Users/smericks/Desktop/forecast_contour.pdf')\n",
    "\n",
    "# TODO: look into changing sampler to dynesty or some other, faster sampler (nested sampling)\n",
    "# TODO: move to sherlock and use MPI\n",
    "# TODO: check silver-only contour (confirm if its wide / constraint is driven by gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HI_medians_table(emcee_chain,param_labels,burnin=1e3):\n",
    "\n",
    "    burnin = int(burnin)\n",
    "    num_params = emcee_chain.shape[2]\n",
    "    chain = emcee_chain[:,burnin:,:].reshape((-1,num_params))\n",
    "\n",
    "    med = np.median(chain,axis=0)\n",
    "    low = np.quantile(chain,q=0.1586,axis=0)\n",
    "    high = np.quantile(chain,q=0.8413,axis=0)\n",
    "    sigma = ((high-med)+(med-low))/2\n",
    "\n",
    "    for i in range(0,num_params):\n",
    "        print(param_labels[i],': ',med[i],' $\\pm$', sigma[i])\n",
    "        \n",
    "HI_medians_table(gold_chain_bright, \n",
    "    ['$H_0$','$\\Omega_M$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "    int(4e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('DataVectors/gold2days_3e3fpd/gold_chain_3e3_LCDM.h5', 'r')\n",
    "gold_chain = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('DataVectors/gold2days_3e3fpd_bright_src/gold_chain_3e3_LCDM.h5', 'r')\n",
    "gold_chain_bright = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('DataVectors/gold2days_silver5days_3e3fpd_bright_src/silver_chain_3e3_LCDM.h5', 'r')\n",
    "silver_chain_bright = h5f.get('mcmc_chain')[:]\n",
    "h5f.close()\n",
    "\n",
    "mu_mean_gold = np.mean(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float))\n",
    "std_mean_gold = np.std(gt_lenses.lens_df.loc[gold,'gamma'].to_numpy().astype(float),ddof=1)\n",
    "\n",
    "\n",
    "exp_chains = [gold_chain_bright,silver_chain_bright]#gold_chain3]#mcmc_chain_quads_w0wa, mcmc_chain_quads_w0wa_1day,mcmc_chain_quads_w0wa_1day_EM, mcmc_chain_quads_w0wa_200]\n",
    "exp_names = ['250 Gold: NPE fpd, m_app_src < 24.','730 Gold+Silver: NPE fpd, m_app_src < 24.']\n",
    "burnin = [int(1500),int(1500),int(1500)]\n",
    "colors = ['goldenrod','indianred']#'gold','silver','indianred','turquoise','purple']\n",
    "custom_labels = []\n",
    "\n",
    "custom_lines = []\n",
    "custom_labels = []\n",
    "for i,exp_chain in enumerate(exp_chains):\n",
    "     \n",
    "    if i ==0:\n",
    "\n",
    "        figure = corner.corner(exp_chain[:,burnin[i]:,:].reshape((-1,4)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','$\\Omega_M$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=300,truths=[70.,0.3,mu_mean_gold,std_mean_gold],truth_color='black',\n",
    "            fig=None,label_kwargs={'fontsize':24},smooth=0.7)\n",
    "\n",
    "    else:\n",
    "\n",
    "        corner.corner(exp_chain[:,burnin[i]:,:].reshape((-1,4)),plot_datapoints=False,\n",
    "            color=colors[i],levels=[0.68,0.95],fill_contours=True,\n",
    "            labels=['$H_0$','$\\Omega_M$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "            dpi=300,truths=[70.,0.3,mu_mean_gold,std_mean_gold],truth_color='black',\n",
    "            fig=figure,label_kwargs={'fontsize':24},smooth=0.7)\n",
    "        \n",
    "    custom_lines.append(Line2D([0], [0], color=colors[i], lw=4))\n",
    "\n",
    "    # calculate h0 constraint\n",
    "    h0, h0_sigma = median_sigma_from_samples(exp_chain[:,burnin[i]:,0].reshape((-1,1)),weights=None)\n",
    "    # construct label\n",
    "    custom_labels.append(exp_names[i]+':\\n $H_0$=%.2f$\\pm$%.2f'%(h0, h0_sigma))\n",
    "\n",
    "\"\"\"\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "bounds = [[63,77],[1.91,2.095],[0.0,0.2]]\n",
    "for r in range(0,3):\n",
    "        for c in range(0,r+1):\n",
    "            if bounds is not None:\n",
    "                axes[r,c].set_xlim(bounds[c])\n",
    "                if r != c :\n",
    "                    axes[r,c].set_ylim(bounds[r])\n",
    "\n",
    "axes = np.array(figure.axes).reshape((3, 3))\n",
    "\"\"\"\n",
    "\n",
    "axes = np.array(figure.axes).reshape((4, 4))\n",
    "axes[0,3].legend(custom_lines,custom_labels,frameon=False,fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_medians_table(gold_chain_bright, \n",
    "    ['$H_0$','$\\Omega_M$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],\n",
    "    1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look further into chains (debugging option) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_by_walker(samples_mcmc, param_mcmc, n_walkers, verbose = False):\n",
    "    n_params = samples_mcmc.shape[2]\n",
    "    n_step = int(samples_mcmc.shape[1])\n",
    "    chain = samples_mcmc\n",
    "    mean_pos = np.zeros((n_params, n_step))\n",
    "    median_pos = np.zeros((n_params, n_step))\n",
    "    std_pos = np.zeros((n_params, n_step))\n",
    "    q16_pos = np.zeros((n_params, n_step))\n",
    "    q84_pos = np.zeros((n_params, n_step))\n",
    "    # chain = np.empty((nwalker, nstep, ndim), dtype = np.double)\n",
    "    for i in np.arange(n_params):\n",
    "        for j in np.arange(n_step):\n",
    "            mean_pos[i][j] = np.mean(chain[:, j, i])\n",
    "            median_pos[i][j] = np.median(chain[:, j, i])\n",
    "            std_pos[i][j] = np.std(chain[:, j, i])\n",
    "            q16_pos[i][j] = np.percentile(chain[:, j, i], 16.)\n",
    "            q84_pos[i][j] = np.percentile(chain[:, j, i], 84.)\n",
    "    fig, ax = plt.subplots(n_params, sharex=True, figsize=(16, 2 * n_params))\n",
    "    if n_params == 1: ax = [ax]\n",
    "    last = n_step\n",
    "    burnin = int((9.*n_step) / 10.) #get the final value on the last 10% on the chain\n",
    "    for i in range(n_params):\n",
    "        if verbose :\n",
    "            print(param_mcmc[i], '{:.4f} +/- {:.4f}'.format(median_pos[i][last - 1], (q84_pos[i][last - 1] - q16_pos[i][last - 1]) / 2))\n",
    "        ax[i].plot(median_pos[i][:last], c='g')\n",
    "        ax[i].axhline(np.median(median_pos[i][burnin:last]), c='r', lw=1)\n",
    "        ax[i].fill_between(np.arange(last), q84_pos[i][:last], q16_pos[i][:last], alpha=0.4)\n",
    "        ax[i].set_ylabel(param_mcmc[i], fontsize=10)\n",
    "        ax[i].set_xlim(0, last)\n",
    "    return fig\n",
    "\n",
    "plot_convergence_by_walker(silver_doublesGOOD[:,100:,:],\n",
    "    ['$H_0$','$\\Omega_M$','w$_0$','w$_a$',r'$\\mu(\\gamma_{lens})$',r'$\\sigma(\\gamma_{lens})$'],#'w$_0$','w$_a$'\n",
    "    20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the assumed training prior match the effective training prior after cuts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_val = pd.read_csv('MassModels/hst_validation_metadata.csv')\n",
    "gamma_vals = hst_val['main_deflector_parameters_gamma'].to_numpy().astype(float)\n",
    "plt.hist(gamma_vals,density=True,label='Samples')\n",
    "x_range = np.arange(1.4,2.6,0.01)\n",
    "mu_est = np.mean(gamma_vals)\n",
    "sigma_est = np.std(gamma_vals,ddof=1)\n",
    "plt.plot(x_range,norm.pdf(x_range,mu_est,sigma_est),label='$\\mu=%.3f$, $\\sigma=%.2f$'%(mu_est,sigma_est))\n",
    "plt.legend()\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.figure()\n",
    "hst_train0 = pd.read_csv('MassModels/hst_train0_metadata.csv')\n",
    "gamma_vals = hst_train0['main_deflector_parameters_gamma'].to_numpy().astype(float)\n",
    "plt.hist(gamma_vals,density=True,label='Samples')\n",
    "x_range = np.arange(1.4,2.6,0.01)\n",
    "mu_est = np.mean(gamma_vals)\n",
    "sigma_est = np.std(gamma_vals,ddof=1)\n",
    "plt.plot(x_range,norm.pdf(x_range,mu_est,sigma_est),label='$\\mu=%.3f$, $\\sigma=%.2f$'%(mu_est,sigma_est))\n",
    "plt.legend()\n",
    "plt.title('Train 0')\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import gaussian_kde\n",
    "plt.figure()\n",
    "\n",
    "gaussian_samples = norm.rvs(mu_est,sigma_est,size=10000)\n",
    "\n",
    "# Let's try a KDE\n",
    "gamma_kde = gaussian_kde(gamma_vals)\n",
    "kde_samples = gamma_kde.resample(size=10000)[0]\n",
    "bins=np.histogram(np.hstack((kde_samples,gamma_vals)), bins=40)[1]\n",
    "\n",
    "counts_exp,_,_ = plt.hist(kde_samples,bins,\n",
    "            histtype='step',label='KDE Estimate')\n",
    "counts_obs,_,_ = plt.hist(gamma_vals,bins,histtype='step',\n",
    "                          label='Training Samples')\n",
    "    \n",
    "# only take bins where counts are greater than 5, then add in an extra bin at beginning and end for the tails\n",
    "idx = np.where((counts_exp > 5) & (counts_obs > 5))[0]\n",
    "# less than some #, bins, greather than some #\n",
    "prepend = np.sum(counts_obs[:idx[0]])\n",
    "append = np.sum(counts_obs[(idx[-1]+1):])\n",
    "counts_obs_final = np.concatenate(([prepend],counts_obs[idx],[append]))\n",
    "prepend = np.sum(counts_exp[:idx[0]])\n",
    "append = np.sum(counts_exp[(idx[-1]+1):])\n",
    "counts_exp_final = np.concatenate(([prepend],counts_exp[idx],[append]))\n",
    "\n",
    "\n",
    "\n",
    "chi2_distance = np.sum((counts_obs_final-counts_exp_final)**2/counts_exp_final)\n",
    "#print(chi2_distance)\n",
    "\n",
    "chi2,_= chisquare(counts_obs_final,counts_exp_final)\n",
    "print(chi2)\n",
    "\n",
    "dof = len(counts_obs_final) - 1\n",
    "print('chi2/dof:', chi2/dof)\n",
    "plt.text(1.6,800,r'$\\frac{\\chi^2}{\\nu}$: %.2f'%(chi2/dof),\n",
    "                        {'fontsize':13})\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
